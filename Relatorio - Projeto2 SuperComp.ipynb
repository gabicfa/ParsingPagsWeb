{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 – Parsing web em C++\n",
    "## Super Computação - 2018/2\n",
    "### Gabriela Almeida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o segundo projeto de super computação trabalhou-se com um problema em que concorrência tem um papel fundamental na obtenção de bom desempenho: download e análise de páginas web. Criou-se um crawler que identifica páginas de produtos em um site de e-commerce e extrai as informações básicas dos produtos. Alguns exemplos são as categorias do site da Magazine Luiza como as seguinte: [DVD Player](https://www.magazineluiza.com.br/dvd-player/tv-e-video/s/et/tvdb/) e [Controle Remoto](https://www.magazineluiza.com.br/controle-remoto/tv-e-video/s/et/cmrt/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para demonstra como a concorrência tem um papel fundamental nesse tipo de problema, esse relatório tem como objetivo apontar a diferença de desempenho entre um crawler sequêncial e um paralelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descrição e Implementação do problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo sequencial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A implementação do modelo sequencial foi feita da seguinte forma: dada uma página de exibição de produto, o web crawler extrai as seguintes informações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. nome do produto\n",
    "    2. descrição do produto\n",
    "    3. url da foto do produto\n",
    "    4. preço à vista \n",
    "    5. preço parcelado\n",
    "    6. categoria do produto\n",
    "    7. url da página de axibição"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A identificação de páginas de produto é feita a partir de sua categoria, ou seja, o crawler desenvolvido é apontado para uma página com os produtos de uma categoria, como os exemplificados anteriormente, e ele consegue obter as páginas de produto, lindando com possíveis paginação da listagem.\n",
    "\n",
    "Após a compilação do programa, explicado no README, o programa é executado na linha de comando como:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ./crawlerSEQ url_da_listagem_por_categoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir dessa url disponibilizada, o programa faz o download do conteúdo html dessa página, usando a biblioteca _curl_: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#include <curl/curl.h>\n",
    "#include <curl/easy.h>\n",
    "#include <curl/curlbuild.h>\n",
    "#include <string>\n",
    "\n",
    "using namespace std;\n",
    "\n",
    "//função auxiliar para a funcao \"download\"\n",
    "size_t WriteCallback(void *contents, size_t size, size_t nmemb, void *userp)\n",
    "{\n",
    "    ((std::string*)userp)->append((char*)contents, size * nmemb);\n",
    "    return size * nmemb;\n",
    "}\n",
    "\n",
    "//Faz o download de uma pagina web a partir de sua url e retorna o seu conteúdo html em formato de string\n",
    "string download(string url) {\n",
    "    CURL *curl;\n",
    "    string readBuffer;\n",
    "\n",
    "    curl = curl_easy_init();\n",
    "    if(curl) {\n",
    "        curl_easy_setopt(curl, CURLOPT_URL, url.c_str());\n",
    "        curl_easy_setopt(curl, CURLOPT_FOLLOWLOCATION, 1L);\n",
    "        curl_easy_setopt(curl, CURLOPT_TRANSFERTEXT, 1L);\n",
    "        curl_easy_setopt(curl, CURLOPT_WRITEFUNCTION, WriteCallback);\n",
    "        curl_easy_setopt(curl, CURLOPT_WRITEDATA, &readBuffer);\n",
    "        curl_easy_perform(curl);\n",
    "        curl_easy_cleanup(curl);\n",
    "\n",
    "        // ofstream myfile;\n",
    "        // myfile.open (\"../page.txt\");\n",
    "        // myfile << readBuffer;\n",
    "        // myfile.close();\n",
    "    }\n",
    "    return readBuffer;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o html da página de categoria disponível, é possível adquirir as urls dos produtos presentes nela e a da página seguinte a ela. Para fazer isso, usou-se o regex da biblioteca boost. Essa ferramenta se baseia em expressões regulares para identificação de strings."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#include <boost/regex.hpp>\n",
    "\n",
    "using namespace std;\n",
    "using namespace boost;\n",
    "\n",
    "//Procura em uma string de entrada str uma string ditada pelo regex reg e retorna os matches, de acordo com o index desejado, pelo vetor result\n",
    "void findMatches(string str, regex reg, vector<string>& result, int index){\n",
    "    smatch matches;\n",
    "    while(regex_search(str, matches, reg)){\n",
    "        //caso exista um match\n",
    "        if(matches.size()> 0){\n",
    "            result.push_back(matches[index]);\n",
    "            str = matches.suffix().str(); \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "//Busca o total de páginas de produtos daquela categoria a partir da primeira página page\n",
    "int totalPages(string page){\n",
    "    vector<string> lastPage;\n",
    "    regex numPages(\"\\\"lastPage\\\":([^,]+)\");\n",
    "    findMatches(page, numPages, lastPage, 1);\n",
    "    int totalPages = stoi(lastPage[0]);\n",
    "    return totalPages;\n",
    "}\n",
    "\n",
    "//Dado uma página de produtos page, o total de página e o número da página atual, procura as urls dos produtos presentes nela e a url para a próxima página\n",
    "vector<string> findMatchesPages(string page, int totalPages, int numPag){\n",
    "    vector<string> urlsProducts;\n",
    "    vector<string> lastPage;\n",
    "\n",
    "    regex href(\"name=\\\"linkToProduct\\\" href=\\\"([^\\\"]+)\");\n",
    "    findMatches(page, href, urlsProducts, 1);\n",
    "\n",
    "    if(numPag != totalPages){\n",
    "        regex nextPage(\"<link rel=\\\"next\\\" href=\\\"([^\\\"]+)\");\n",
    "        findMatches(page, nextPage, lastPage, 1);\n",
    "        lastPage[0] = \"https://www.magazineluiza.com.br\"+lastPage[0];\n",
    "        urlsProducts.push_back(lastPage[0]);\n",
    "        return urlsProducts;\n",
    "    }\n",
    "    //Caso seja a última página, não tem próxima página\n",
    "    else{\n",
    "        urlsProducts.push_back(\"none\");\n",
    "        return urlsProducts;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sendo possível adquirir todas as urls das páginas de produtos da categoria ,além das url dos produtos dessas páginas, então é possível fazer o download das páginas de todos os produtos e obter as informações desejadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O resultado do programa é escrito na saída padrão no formato json. Cada produto é um objeto com os seguintes campos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    {\n",
    "        \"nome\": \"\",\n",
    "        \"descricao\": \"\",\n",
    "        \"foto\": \"\",\n",
    "        \"preco: 0,\n",
    "        \"preco_parcelado\": 0,\n",
    "        \"preco_num_parcelas\": 0,\n",
    "        \"categoria\": \"\",\n",
    "        \"url\": \"\"\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "//A partir da página de produto page é possivel extrair as informações necessárias, a url do produto já é informação adquirida anteriormente\n",
    "\n",
    "void collectProduct(string page, string url){\n",
    "    vector<string> buffer;\n",
    "\n",
    "    regex name (\"<h1 class=\\\"header-product__title\\\" itemprop=\\\"name\\\">([^<]+)\");\n",
    "    findMatches(page, name, buffer, 1);\n",
    "    string productName = buffer[0];\n",
    "\n",
    "    buffer.clear();\n",
    "    regex desc(\"<h2 class=\\\"description__product-title\\\">([^<]+)</h2>    <p class=\\\"description__text\\\"></p>([^<]+)\");\n",
    "    findMatches(page, desc, buffer, 2);\n",
    "    string productDescription = buffer[0];\n",
    "\n",
    "    buffer.clear();\n",
    "    regex image(\"showcase-product__big-img js-showcase-big-img\\\" src=\\\"(https[^\\\"]+)\");\n",
    "    findMatches(page, image, buffer, 1);\n",
    "    string productImage = buffer[0];\n",
    "\n",
    "    buffer.clear();\n",
    "    regex price(\"price-template__text[^>]+>([^<]+)</span>\");\n",
    "    findMatches(page, price, buffer, 1);\n",
    "    string productPrice = buffer[0];\n",
    "\n",
    "    buffer.clear();\n",
    "    regex parcelado(\"installmentAmount\\\": \\\" ([^\\\"]+)\");\n",
    "    findMatches(page, parcelado, buffer, 1);\n",
    "    string precoParcelado = buffer.front();\n",
    "\n",
    "    buffer.clear();\n",
    "    regex numparcelas(\"installmentQuantity\\\": \\\"([^\\\"]+)\");\n",
    "    findMatches(page, numparcelas, buffer, 1);\n",
    "    string numeroParcelas = buffer.front();\n",
    "\n",
    "    buffer.clear();\n",
    "    regex category(\"itemprop=\\\"item\\\"> ([^>]+)</a>  </li>  </ul>\");\n",
    "    findMatches(page, category, buffer, 1);\n",
    "    string productCategory = buffer.front();\n",
    "\n",
    "    string out = \n",
    "    \"  {\\n\"\n",
    "    \"    \\\"nome\\\" : \\\"\" + productName +\"\\\",\\n\"\n",
    "    \"    \\\"descricao\\\" : \\\"\" + productDescription +\"\\\",\\n\"\n",
    "    \"    \\\"foto\\\" : \\\"\" + productImage +\"\\\",\\n\"\n",
    "    \"    \\\"preco\\\" : \\\"\" + productPrice +\"\\\",\\n\"\n",
    "    \"    \\\"preco_parcelado\\\" : \\\"\" + precoParcelado +\"\\\",\\n\"\n",
    "    \"    \\\"preco_num_parcelas\\\" : \\\"\" + numeroParcelas +\"\\\",\\n\"\n",
    "    \"    \\\"categoria\\\" : \\\"\" + productCategory +\"\\\",\\n\"\n",
    "    \"    \\\"url\\\" : \\\"\" + url +\"\\\",\\n\"\n",
    "    \"  },\\n\";\n",
    "\n",
    "    cout << out;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo paralelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A implementação do modelo paralelo possui uma lógica parecida com a do sequencial. Assim como o modelo anterior, é feito download da página com os produtos, são identificadas as urls dos produtos presentes nela e a url da página seguinte, é feito o download das paginas dos produtos, extraídas as informações necessárias e após a análise de todos os produtos faz-se o mesmo processo com a página seguinte.\n",
    "\n",
    "No entanto essa implementação é dada em paralelismo por tarefas, ou seja, é preciso sincronizar tarefas que dependem parcialmente umas das outras (parte de uma tarefa depende de um resultado de outra quanto o restante é independente). Ao exercutar as partes independente de maneira paralela pode-se obter ganhos de desempenho consideráveis em processos longos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo paralelo por tarefas foi estruturado da seguinte forma:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imagem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível perceber no esquema a cima que as threads compartilham de alguns recursos fazendo com que elas sejam interdependentes.\n",
    "\n",
    "A thread prodLinkCollectorThread é encarregada de fazer o download das páginas com os produtos, identificar as urls dos produtos presentes nelas e colocá-las na listofUrls. Além disso essa thread é encarregada de adquirir a url da pagina com produtos seguinte e fazer o mesmo processo, sucessivamente, até a última página dessa categoria. Desse modo, essa thread terá adicionado à listofUrls as urls de todos os produtos disponíveis naquela categoria.\n",
    "A segunda thread, prodPageCollectorThread, precisa recolher uma url da listofUrls, fazer o download dela e adicionar-la à listofPages. \n",
    "Já prodCollectorThread coleta uma pagina de listofPages, coleta as informações desejadas e as adiciona a uma string global \"finalJSON\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Levando em consideração essa estruturação, é possível definir multiplas threads do tipo prodPageCollectorThread fazendo com que elas rodem em paralelo.O mesmo pode acontecer com prodCollectorThread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como essa threads compartilham algumas estruturas, o uso de semáforos é fundamental para coordenar o acesso de cada thread a elas. Desse modo foram definidos os seguintes semáforos:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Semaphore accessListofUrls(1);\n",
    "Semaphore accessListofPages(1);\n",
    "Semaphore accessJSON(1);\n",
    "Semaphore listofUrlCount(0);\n",
    "Semaphore listofPagesCount(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O primeiro controla o acesso à listofUrls, o segundo o acesso à listofPages e o tercero o acesso à string global \"finalJSON\". Já os dois últimos valem como contadores de elementos em listofUrls e listofPages respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim como no sequencial, após a compilação do programa, explicado no README, o programa é executado na linha de comando como:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ./crawlerPAR url_da_listagem_por_categoria numProducers numConsumers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No qual numProducers é o número de threads do tipo prodPageCollectorThread e numConsumers é o numero de threads do tipo prodCollectorThread.\n",
    "\n",
    "As threads são definidas da seguinte forma:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "string url = argv[1];\n",
    "    int numProducers = atoi(argv[2]);\n",
    "    int numConsumers = atoi(argv[3]);\n",
    "\n",
    "    thread prodPageCollectorThread[numProducers];\n",
    "    thread prodCollectorThread[numConsumers];\n",
    "\n",
    "    thread prodLinkCollectorThread(produceUrls, std::ref(listofUrls), std::ref(accessListofUrls), std::ref(listofUrlCount), url, std::ref(noMoreUrls));\n",
    "    for(int p=0; p<numProducers; p++){\n",
    "        prodPageCollectorThread[p] = thread(producePages, std::ref(listofUrls), std::ref(accessListofUrls), std::ref(listofUrlCount), std::ref(listofPages), std::ref(accessListofPages), std::ref(listofPagesCount), std::ref(noMoreUrls), std::ref(noMorePages), numProducers);\n",
    "    }\n",
    "    for(int c=0; c<numConsumers; c++){\n",
    "        prodCollectorThread[c] = thread(consumePages,  std::ref(listofPages), std::ref(accessListofPages), std::ref(listofPagesCount), std::ref(accessJSON), std::ref(finalJSON), std::ref(noMorePages),numConsumers);\n",
    "    }\n",
    "\n",
    "    prodLinkCollectorThread.join();\n",
    "    for(int p=0; p<numProducers; p++){\n",
    "        prodPageCollectorThread[p].join();\n",
    "    }\n",
    "    for(int c=0; c<numConsumers; c++){\n",
    "        prodCollectorThread[c].join();\n",
    "    }"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "/*FUNÇÃO prodLinkCollectorThread - Faz o download das páginas com os produtos, identifica as urls dos produtos presentes nelas e coloca-as na listofUrls. Além disso adquire a url da pagina com produtos seguinte e fazer o mesmo processo, sucessivamente, até a última página dessa categoria.*/\n",
    "void produceUrls(list<string>& listofUrls, Semaphore& accessListofUrls, Semaphore& listofUrlCount, string url, bool& noMoreUrls){\n",
    "    list<string> urlsPage;\n",
    "    string nextPageUrl;\n",
    "    \n",
    "    string page = download(url);\n",
    "    int total = totalPages(page);\n",
    "\n",
    "    for(int p=1; p<=total; p++){\n",
    "        urlsPage = findMatchesPages(page, total, p);\n",
    "        nextPageUrl = urlsPage.back();\n",
    "        urlsPage.pop_back();\n",
    "        accessListofUrls.acquire();\n",
    "            for(auto u = urlsPage.begin(); u != urlsPage.end(); ++u){\n",
    "                listofUrls.push_back(*u);\n",
    "                listofUrlCount.release();\n",
    "            }\n",
    "        accessListofUrls.release();\n",
    "        \n",
    "        page = download(nextPageUrl);\n",
    "    }\n",
    "    noMoreUrls = true;\n",
    "}\n",
    "\n",
    "/*FUNÇÃO prodPageCollectorThread - Recolhe* uma url da listofUrls, faz o download dela e a adicionar à listofPages*/\n",
    "void producePages(list<string>&listofUrls, Semaphore& accessListofUrls, Semaphore& listofUrlCount, list<string>&listofPages, Semaphore& accessListofPages, Semaphore& listofPagesCount, bool& noMoreUrls, bool& noMorePages, int numProducers){\n",
    "    string currentProductUrl;\n",
    "    string currentProductPage;\n",
    "    bool end = false;\n",
    "    while(!end){\n",
    "        listofUrlCount.acquire();\n",
    "        accessListofUrls.acquire();\n",
    "            if(noMoreUrls && listofUrls.empty()){\n",
    "                end = true;\n",
    "            }\n",
    "            else{\n",
    "                currentProductUrl = listofUrls.front();\n",
    "                listofUrls.pop_front();\n",
    "                if(noMoreUrls && listofUrls.empty()){\n",
    "                    for(int pt=0; pt< numProducers; pt++){\n",
    "                        listofUrlCount.release();\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        accessListofUrls.release();\n",
    "        \n",
    "        urrentProductPage = download(currentProductUrl);\n",
    "        \n",
    "        accessListofPages.acquire();\n",
    "            listofPages.push_back(currentProductPage);\n",
    "            listofPagesCount.release();\n",
    "        accessListofPages.release();\n",
    "    }\n",
    "    noMorePages = true;\n",
    "}\n",
    "\n",
    "/*FUNÇÃO prodCollectorThread - coleta uma pagina de listofPages, coleta as informações desejadas e as adiciona a string global \"finalJSON\"*/\n",
    "void consumePages(list<string>& listofPages, Semaphore& accessListofPages, Semaphore& listofPagesCount, Semaphore& accessJSON, const string &finalJSON, bool& noMorePages, int numConsumers){\n",
    "    string currentProductPage;\n",
    "    string jsonProduct;\n",
    "    string* tempJson = const_cast<string*>(&finalJSON);\n",
    "    bool end = false;\n",
    "\n",
    "    while(!end){\n",
    "        listofPagesCount.acquire();\n",
    "        accessListofPages.acquire();\n",
    "            if(noMorePages && listofPages.empty()){\n",
    "                end = true;\n",
    "            }\n",
    "            else{\n",
    "                currentProductPage = listofPages.front();\n",
    "                listofPages.pop_front();\n",
    "                if(noMorePages && listofPages.empty()){\n",
    "                    for(int ct=0; ct< numConsumers; ct++){\n",
    "                        listofPagesCount.release();\n",
    "                    }\n",
    "                }\n",
    "            };\n",
    "        accessListofPages.release();\n",
    "        \n",
    "        jsonProduct = collectProduct(currentProductPage);\n",
    "        \n",
    "        accessJSON.acquire();\n",
    "            *tempJson = finalJSON + jsonProduct;\n",
    "        accessJSON.release();\n",
    "    }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
